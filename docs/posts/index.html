<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width">
  <title>Thamme Gowda</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Tangerine">-->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.5/css/bulma.min.css">
    <link rel="stylesheet" href="https://isi.edu/~tg/theme/css/main.557acf99.css">
  <style media="print">.is-hidden-print{display:none !important}</style>
      <link rel="stylesheet" href="https://isi.edu/~tg/css/custom.css">
</head>

<body id="index" class="home">
<header class="hero is-primary">
  <div class="hero-head">
    <div class="container">
      <nav class="navbar">
        <div class="navbar-brand">
          <a class="navbar-item title is-3"
             href="https://isi.edu/~tg/">Thamme Gowda</a>
        </div>
      </nav>
    </div>
  </div>
</header>

<nav class="navbar has-shadow is-hidden-print">
  <div class="container">
    <div class="navbar-center"></div>
    <span id="navToggle" class="navbar-burger">
      <span></span>
      <span></span>
      <span></span>
    </span>
    <div id="navMenu" class="navbar-menu">
        <div class="navbar-end">
            <a class="navbar-item is-tab" href="https://isi.edu/~tg">Home</a>
            <a class="navbar-item is-tab" href="https://isi.edu/~tg/posts">Blog</a>
            <a class="navbar-item is-tab" href="https://isi.edu/~tg/software">Software</a>
            <a class="navbar-item is-tab" href="https://isi.edu/~tg/publications.html">Publications</a>
            <a class="navbar-item is-tab" href="https://isi.edu/~tg/notes">Notes</a>
        </div>
    </div>
  </div>
</nav>

<div class="container">
  <div class="section columns">
    <article class="column is-four-fifths-desktop is-three-quarter-tablet has-text-justified">
      <aside id="featured" class="body">
        <article>
          <h1 class="title">
            <a href="https://isi.edu/~tg/posts/2021/03/macroavg-rare-types-important/">Macro-Average: Rare Types Are Important Too</a>
          </h1>
<footer class="post-info">
  <abbr class="published" title="2021-03-11T10:20:00-08:00">
    Published <span class="is-info">11 March 2021</span>
    in <a href="https://isi.edu/~tg/category/paper.html">Paper</a>
  </abbr>

    <p class="author">
      <em>by           <a class="url fn" href="https://isi.edu/~tg/author/thamme-gowda.html">Thamme Gowda <tg@isi.edu></a>
</em>
      <span class="tag is-small is-rounded">
        <a href="https://isi.edu/~tg/tag/nmt.html">NMT</a>
      </span>
    </p>
  
</footer>          <div class="section">
              <div class="sect1">
<h2 id="_abstract">Abstract</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases.</p>
</div>
<div class="paragraph">
<p>We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods' outputs.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_links">Links</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>DOI (To Appear at NAACL 2021)</p>
</li>
<li>
<p>Paper: <a href="https://arxiv.org/abs/2104.05700" class="bare">https://arxiv.org/abs/2104.05700</a>  <a href="https://arxiv.org/pdf/2104.05700.pdf" target="_blank" rel="noopener"><span class="icon red"><i class="fa fa-download fa-2x"></i></span></a></p>
</li>
<li>
<p>Code</p>
<div class="ulist">
<ul>
<li>
<p>Analysis: <a href="https://github.com/thammegowda/007-mt-eval-macro" class="bare">https://github.com/thammegowda/007-mt-eval-macro</a></p>
</li>
<li>
<p>MacroF1 code is in a fork of sacrebleu <a href="https://github.com/isi-nlp/sacrebleu" class="bare">https://github.com/isi-nlp/sacrebleu</a><br></p>
</li>
<li>
<p><a href="https://github.com/mjpost/sacrebleu/pull/153">Pull request is submitted to sacrebleu</a><br>
Usage of MacroF1 and MicroF1:<br></p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">sacrebleu $REF.txt -m macrof &lt; $HYP.detok
sacrebleu $REF.txt -m microf &lt; $HYP.detok</code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary">Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://isi.edu/~tg/posts/2020/11/2020-optimal-vocab-nmt/">In the previous work</a>, we framed NMT as a multi-class classifier. In this work, we evaluate NMT (or MT, NLG in general) as if it is a multi-class classifier. The overall performance of a multi-class classifier is commonly obtained by taking an average of individual class performances. Two common ways to compute averages are Micro- and Macro- averages. In many tasks used by academia, test set classes are often balanced, and in those scenarios micro and macro are essentially equivalent. However, if the test set classes are imbalanced, which is likely the case in real-world, micro and macro averages make a huge difference depending on the degree of imbalance.</p>
</div>
<div class="paragraph">
<p>In NLP/linguistic terms, micro average assigns equal weight to each token (or instance), whereas macro-average assigns equal weight to each type (or class). Since the types in natural language datasets resemble Zipfian distribution, an extremely imbalanced distribution, reporting micro-averaged performance would be misleading, especially because minority classes (i.e rare types) are equally or more important than majority classes (aka stop words).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/brown-corpus-zipf.png" alt="brown corpus zipf">
</div>
<div class="title">Figure 1. Type frequencies as seen on Brown Coprus show an exteme imbalance. A few types dominate the corpus where as a vast number of types are infrequent</div>
</div>
<div class="paragraph">
<p><em>Do we really need one more MT evaluation metric? What is wrong with the current metrics that needs to be addressed?</em> BLEU[1], ChrF[2], and such model-free metrics that are in use today are micro-averaged metrics&#8201;&#8212;&#8201;they treat each token equally. As a result, they provide higher rewards for learning stopword types like <em>if</em>, <em>and</em>, and <em>but</em>, and significantly less reward for learning to generate content words like <em>xylophone</em>, <em>peripatetic</em>, and <em>defenestrate</em>. In other words, when we measure using micro-averaged metrics the improvements in translating content words better than baselines, we see the delta compared to baseline is so tiny; we tend to believe that the improvements are not significant. And we risk in discarding good ideas because the micro-averaged metrics marginalize rare types.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/2021-macroavg-vs-others.png" alt="2021 macroavg vs others">
</div>
<div class="title">Figure 2. The ‘the’ type appears 3019 times in DE-EN NewsTest19 test corpus, and zero-recall of ‘the’ results in a losses up to 18.83% BLEU, 9.38% ChrF1, 6.45% MicroF1, and 0.03% MacroF1.</div>
</div>
<div class="paragraph">
<p>But in reality, these low frequency content types carry a lot of information (isnt it why they called content words?). Information theory also agrees with this statement</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/brown-corpus-infocontent.png" alt="brown corpus infocontent">
</div>
<div class="title">Figure 3. Rare types contain more information content than frequent types.</div>
</div>
<div class="paragraph">
<p>On the other hand, a lot of model-based evaluation metrics have been proposed lately. These high performance models are opaque, expensive, and biased in several undesirable ways. For example, we looked into BLEURT[3], one of the best model-based metrics at the time of writing, and it showed preferences on certain names in an undesirable ways. In addition, the scores are not really interpretable.</p>
</div>
<table class="tableblock frame-none grid-none stretch">
<caption class="title">Table 1. Scores are from BLEURT base model, and demonstrate the undesirable biases of model-based metrics. Model-free metrics such as BLEU and MacroF1 would consider each hypothesis as equally wrong.</caption>
<colgroup>
<col style="width: 13.3333%;">
<col style="width: 6.6666%;">
<col style="width: 80.0001%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Reference:</p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock">You must be a doctor.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hypothesis:</p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock">__ must be a doctor.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">He</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.74</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Joe</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.98</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sue</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-1.04</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">She</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-1.10</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Reference:</p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock">It is the greatest country in the world.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hypothesis:</p></td>
<td class="tableblock halign-left valign-top" colspan="2"><p class="tableblock">__ is the greatest country in the world.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">France</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.02</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">America</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.06</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Russia</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.16</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Canada</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.31</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Will a simple metric like MacroF1 be competitive for MT evaluation?</strong> We have been very skeptical too. So we&#8217;ve tested it thoroughly on several tasks, which are as follows:</p>
</div>
<div class="paragraph">
<p>On <strong>WebNLG dataset</strong>[4] which has human annotations for fluency, grammar, and semantics, MacroF1 found to be a weak indicator of fluency and grammar, but a strong indicator of semantics. Even ChrF1 has a similar property: a weak indicator of fluency and grammar compared to BLEU, but a strong indicator of semantics. Since MicroF1 weighs towards frequent types that contribute to fluency and grammar, it scores relatively higher in fluency compared to MacroF1 but relatively lower on semantics.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/2021-macroavg-webnlg.png" alt="2021 macroavg webnlg">
</div>
<div class="title">Figure 4. On WebNLG dataset, MacroF1 is found to be a poor indicator of Fluency and Grammar, but a strong indicator of Semantics.</div>
</div>
<div class="paragraph">
<p><strong>WMT Metrics</strong>[5] datasets have human judgements for tens of languages. Here MacroF1 rarely scored the best correlations in 2017. It got better in 2018. And, it scored the highest number of wins per metric in 2019. This trend is interesting; especially if you wonder how a metric that is a poor indicator of fluency and grammar (as seen on our findings on WebNLG datasets) can get such good results on the most recent WMT Metrics Task? We believe NMT models have made great progress in the recent years. The latest models can produce very fluent translations. Now we are in the era where semantics is a key discriminating factor. MacroF1 can capture adequacy better than alternatives.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/2021-macroavg-WMT-metrics.png" alt="2021 macroavg WMT metrics">
</div>
<div class="title">Figure 5. On WMT Metrics task, MacroF1 wins in the recent year.</div>
</div>
<div class="paragraph">
<p>We also tested MT metrics on <strong>downstream task of cross-lingual information retrieval (CLIR)</strong>. This task focused more adequacy and less on fluency. We used CLSSTS 2020 datasets[6] for Lithuanian-English, Pashto-English, and Bulgarian-English and mesaured the IR performance using mean average precision (mAP). This is an IR task in which queries are in English but documents are in foreign languages. MacroF1 found to be the strongest indicator of IR task performance.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/2021-macroavg-CLIR.png" alt="2021 macroavg CLIR">
</div>
<div class="title">Figure 6. On a CLSSTS 2020 CLIR task, MacroF1 showed strongest correlations with IR task performance across three languages.</div>
</div>
<div class="paragraph">
<p>Next, we used MacroF1 to analyse the differences between unsupervised (UNMT) and supervised NMT (SNMT). In the recent years, UNMT has shown very promising results. In many cases, UNMT has shown to achieve BLEU scores comparable with SNMT models. So we took a bunch of UNMT and SNMT models that have comparable BLEU scores and looked at their MacroF1 scores. Even though UNMT models have a comparable BLEU scores they are lagging behind SNMT by considerable margin in terms of MacroF1.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/2021-macroavg-snmt-unmt-diff.png" alt="2021 macroavg snmt unmt diff">
</div>
<div class="title">Figure 7. Even though SNMT and UNMT achieve comparable BLEU scores, they differ significantly in terms of MacroF1.</div>
</div>
<div class="paragraph">
<p>As an added bonus, MacroF1 score can be broken down into individual type/class F1 scores. We looked at how the performance varies across all the types in vocabulary. On high frequency types, UNMT models are relatively better (i.e. better F1 score) than SNMT, which results in fluent outputs, hence good BLEU scores, but UNMT is relatively poorer in translating low frequency types, hence lower MacroF1 than SNMT.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://isi.edu/~tg/images/2021-macroavg-snmt-unmt-enfr.png" alt="2021 macroavg snmt unmt enfr">
</div>
<div class="title">Figure 8. SNMT vs UNMT MacroF1 on the most frequent 500 types on EN-FR test set. UNMT outperforms SNMT on frequent types, howver, SNMT is generally better than UNMT on rare types. This trend is similar on the other languages we tested: FR-EN, EN-DE, DE-EN, EN-RO, and RO-EN.</div>
</div>
<div class="paragraph">
<p><em>To learn more about this work, please refer to our paper. Send any questions to <code>tg(at)isi.edu</code>.</em></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_citation">Citation</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="content">
<pre>% Arxiv preprint
@misc{gowda2021macroaverage,
      title={Macro-Average: Rare Types Are Important Too},
      author={Thamme Gowda and Weiqiu You and Constantine Lignos and Jonathan May},
      year={2021},
      eprint={2104.05700},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% To Appear at NAACL-HLT 2021:
@inproceedings{gowda-etal-2021-macro-average,
    title = "Macro-Average: Rare Types Are Important Too",
    author = "Gowda, Thamme and
    You, Weiqiu and
    Lignose, Constantine and
    May, Jonathan ",
    booktitle = "The Association for Computational Linguistics: NAACL-HLT 2021",
    publisher = "Association for Computational Linguistics",
    month = jun,
    year = "2021",
    address = "Online",
}</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_acknowledgements">Acknowledgements</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Thanks to Shantanu Agarwal, Joel Barry, and Scott Miller for their help with CLSSTS CLIR experiments, and Daniel Cohen for the valuable discussions on IR evaluation metrics.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_references">References</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. AssociationforComputationalLinguistics,Philadelphia,Pennsylvania,USA,311–318. <a href="https://doi.org/10.3115/1073083.1073135" class="bare">https://doi.org/10.3115/1073083.1073135</a></p>
</li>
<li>
<p>Maja Popović. 2015. ChrF: Character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation. Association for Computational Linguistics, Lisbon, Portugal, 392–395. <a href="https://doi.org/10.18653/v1/W15-3049" class="bare">https://doi.org/10.18653/v1/W15-3049</a></p>
</li>
<li>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning Robust Metrics for Text Generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,Online,7881–7892. <a href="https://www.aclweb.org/anthology/2020.acl-main.704" class="bare">https://www.aclweb.org/anthology/2020.acl-main.704</a> <a href="https://github.com/google-research/bleurt" class="bare">https://github.com/google-research/bleurt</a></p>
</li>
<li>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating Training Corpora for NLG Micro-Planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume1:LongPapers).AssociationforComputationalLinguistics,179–188. <a href="https://doi.org/10.18653/v1/P17-1017" class="bare">https://doi.org/10.18653/v1/P17-1017</a> <a href="https://gitlab.com/webnlg/webnlg-human-evaluation" class="bare">https://gitlab.com/webnlg/webnlg-human-evaluation</a></p>
</li>
<li>
<p>Qingsong Ma, Johnny Wei, Ondřej Bojar, and Yvette Graham. 2019. Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). Association for Computational Linguistics, Florence, Italy, 62–90. <a href="http://www.aclweb.org/anthology/W19-5302" class="bare">http://www.aclweb.org/anthology/W19-5302</a>  <a href="http://www.statmt.org/wmt19/metrics-task.html" class="bare">http://www.statmt.org/wmt19/metrics-task.html</a></p>
</li>
<li>
<p>Ilya Zavorin, Aric Bills, Cassian Corey, Michelle Morrison, Audrey Tong, and Richard Tong. 2020. Corpora for Cross- Language Information Retrieval in Six Less-Resourced Languages. In Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020). European Language Resources Association, Marseille, France, 7–13. <a href="https://www.aclweb.org/anthology/2020.clssts-1.2" class="bare">https://www.aclweb.org/anthology/2020.clssts-1.2</a> <a href="http://users.umiacs.umd.edu/~oard/clssts/" class="bare">http://users.umiacs.umd.edu/~oard/clssts/</a></p>
</li>
</ol>
</div>
</div>
</div>
          </div>
        </article>
      </aside>
  <div class="is-hidden-print">
    <h2 class="subtitle">Other articles</h2>
    <div class="columns is-multiline">
        <div class="column is-half-tablet is-one-third-desktop">
          <div class="card is-fullwidth is-fullheight">
            <div class="card-content">
              <a href="https://isi.edu/~tg/posts/2020/11/2020-optimal-vocab-nmt/">
                <h3 class="title is-5">Finding the Optimal Vocabulary for Neural Machine Translation</h3>
                <div class="heading subtitle">01 November 2020</div>
                <div><p>Finding the Optimal Vocabulary for Neural Machine Translation</p></div>
              </a>
            </div>
          </div>
        </div>
    </div>
<nav class="pagination is-centered">
    <a class="pagination-previous" href="#" disabled>
      Previous
    </a>
    <a class="pagination-next" href="#" disabled>
      Next page
    </a>
  <ul class="pagination-list">
    <li>Page 1 / 1</li>
  </ul>
</nav>
<br>  </div>
    </article>

    <div class="column is-one-fifth-desktop is-one-quarter-tablet is-hidden-print">
      <aside class="menu">
          <p class="menu-label">Links</p>
          <ul class="menu-list">
              <li><a href="https://usc.edu/" target="_blank">
                <span class="icon is-small"><i class="fa fa-globe fa-fw"></i></span>
                <span class="link-text is-size-7">USC</span>
              </a></li>
              <li><a href="https://isi.edu/" target="_blank">
                <span class="icon is-small"><i class="fa fa-globe fa-fw"></i></span>
                <span class="link-text is-size-7">USC ISI</span>
              </a></li>
              <li><a href="https://viterbischool.usc.edu/" target="_blank">
                <span class="icon is-small"><i class="fa fa-globe fa-fw"></i></span>
                <span class="link-text is-size-7">USC Viterbi</span>
              </a></li>
              <li><a href="https://cs.usc.edu/" target="_blank">
                <span class="icon is-small"><i class="fa fa-globe fa-fw"></i></span>
                <span class="link-text is-size-7">USC CS Dept</span>
              </a></li>
              <li><a href="https://www.isi.edu/research_groups/nlg/home" target="_blank">
                <span class="icon is-small"><i class="fa fa-globe fa-fw"></i></span>
                <span class="link-text is-size-7">USC ISI-NLP</span>
              </a></li>
          </ul>
<p class="menu-label">Social</p>
<ul class="menu-list">
    <li><a href="mailto:" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-envelope fa-fw"></i>
      </span>
      <span class="link-text is-size-7">tg@isi.edu</span>
    </a></li>
    <li><a href="https://twitter.com/thammegowda" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-twitter fa-fw"></i>
      </span>
      <span class="link-text is-size-7">Twitter</span>
    </a></li>
    <li><a href="https://github.com/thammegowda" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-github fa-fw"></i>
      </span>
      <span class="link-text is-size-7">Github</span>
    </a></li>
    <li><a href="https://www.linkedin.com/in/thammegowda/" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-linkedin fa-fw"></i>
      </span>
      <span class="link-text is-size-7">LinkedIn</span>
    </a></li>
    <li><a href="https://stackexchange.com/users/1632148/thamme-gowda?tab=accounts" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-globe fa-fw"></i>
      </span>
      <span class="link-text is-size-7">StackOverflow</span>
    </a></li>
    <li><a href="https://www.quora.com/profile/Thamme-Gowda" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-quora fa-fw"></i>
      </span>
      <span class="link-text is-size-7">Quora</span>
    </a></li>
    <li><a href="https://instagram.com/thammegowda/" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-instagram fa-fw"></i>
      </span>
      <span class="link-text is-size-7">Instagram</span>
    </a></li>
    <li><a href="https://www.goodreads.com/user/show/31845074-thamme-gowda" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-globe fa-fw"></i>
      </span>
      <span class="link-text is-size-7">GoodReads</span>
    </a></li>
    <li><a href="https://thammegowda.wordpress.com" target="_blank">
      <span class="icon is-small">
          <i class="fa fa-wordpress fa-fw"></i>
      </span>
      <span class="link-text is-size-7">WordPress</span>
    </a></li>


</ul>      </aside>
    </div>
  </div>
</div>

<footer class="footer" style="padding: 1rem 1rem 1rem;">
  <div class="container has-text-centered">
    <!--<div class="credits">-->
      <span class="is-size-7">©2021 Thamme Gowda. This site is made with <a href="https://blog.getpelican.com/">Pelican</a>
        / <a href="https://github.com/textbook/bulrush">Bulrush </a> / <a href="https://bulma.io">Bulma</a>.</span>
    <!--</div>-->
  </div>
  <!---->
</footer>

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-109985365-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = 'https://ssl.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
  document.getElementById('navToggle').addEventListener('click', function () {
    var nav = document.getElementById('navMenu');
    var className = nav.getAttribute('class');
    if (className == 'navbar-menu') {
      nav.className = 'navbar-menu is-active';
    } else {
      nav.className = 'navbar-menu';
    }
  });
</script>
</body>
</html>